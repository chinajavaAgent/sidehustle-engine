#!/usr/bin/env python3
"""
RPAå¤šå¹³å°å†…å®¹çˆ¬è™«ç³»ç»Ÿ
åŸºäºæµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ€æœ¯ï¼Œæ— éœ€APIå¯†é’¥ï¼Œæ”¯æŒGoogleã€YouTubeã€Redditã€Twitterç­‰å¹³å°çš„é«˜çº§æœç´¢
ä½¿ç”¨Playwrightè¿›è¡ŒçœŸå®æµè§ˆå™¨æ“ä½œ
"""

import asyncio
import re
import random
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime
import logging
from urllib.parse import quote

try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

from .rpa_anti_detection import RPAAntiDetection

@dataclass
class RPAContentItem:
    """RPAæŠ“å–çš„å†…å®¹é¡¹æ•°æ®ç»“æ„"""
    platform: str
    title: str
    content: str
    url: str
    author: str
    engagement_metrics: Dict[str, Any]  # å¹³å°ç‰¹å®šçš„å‚ä¸åº¦æŒ‡æ ‡
    confidence_score: float             # å†…å®¹ç½®ä¿¡åº¦è¯„åˆ† (0-1)
    scraped_at: str                    # æŠ“å–æ—¶é—´

class RPAMultiPlatformCrawler:
    """RPAå¤šå¹³å°å†…å®¹çˆ¬è™« - åŸºäºæµè§ˆå™¨è‡ªåŠ¨åŒ–"""
    
    def __init__(self, use_playwright: bool = True):
        self.logger = logging.getLogger(__name__)
        self.use_playwright = use_playwright and PLAYWRIGHT_AVAILABLE
        self.anti_detection = RPAAntiDetection()
        
        # æµè§ˆå™¨å®ä¾‹
        self.browser = None
        self.context = None
        
        # RPAæœç´¢é…ç½®
        self.search_limits = {
            'google': 15,
            'youtube': 10,
            'reddit': 15,
            'twitter': 10
        }
        
        # æœç´¢å»¶è¿Ÿé…ç½®
        self.delays = {
            'between_searches': (3, 7),
            'page_load': (2, 5)
        }
    
    async def __aenter__(self):
        await self.initialize_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.cleanup_browser()
    
    async def initialize_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        try:
            if self.use_playwright:
                self.browser, self.context = await self.anti_detection.create_stealth_browser_playwright()
                self.logger.info("Playwrightæµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                raise NotImplementedError("å½“å‰ç‰ˆæœ¬ä¸»è¦æ”¯æŒPlaywright")
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def cleanup_browser(self):
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        try:
            if self.browser:
                await self.browser.close()
            self.logger.info("æµè§ˆå™¨èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.warning(f"æµè§ˆå™¨æ¸…ç†è­¦å‘Š: {e}")
    
    async def search_all_platforms_rpa(self, query: str, time_range: str = "7d") -> List[RPAContentItem]:
        """
        ä½¿ç”¨RPAåœ¨æ‰€æœ‰å¹³å°ä¸Šæœç´¢å†…å®¹
        
        Args:
            query: æœç´¢å…³é”®è¯
            time_range: æ—¶é—´èŒƒå›´ (1d, 7d, 30d)
        """
        self.logger.info(f"å¼€å§‹RPAå¤šå¹³å°æœç´¢: {query}")
        
        all_content = []
        # ä»ç¨³å®šçš„å¹³å°å¼€å§‹ï¼ŒYouTubeæš‚æ—¶è·³è¿‡
        platforms = ['google', 'reddit']  # ä¼˜å…ˆæµ‹è¯•ç¨³å®šå¹³å°
        
        # ä¾æ¬¡æœç´¢å„å¹³å°ï¼ˆé¿å…å¹¶å‘å¯¼è‡´æ£€æµ‹ï¼‰
        for platform in platforms:
            try:
                self.logger.info(f"æ­£åœ¨æœç´¢ {platform.upper()}...")
                
                if platform == 'google':
                    results = await self.search_google_rpa(query, time_range)
                elif platform == 'youtube':
                    results = await self.search_youtube_rpa(query, time_range)
                elif platform == 'reddit':
                    results = await self.search_reddit_rpa(query, time_range)
                
                all_content.extend(results)
                
                # å¹³å°é—´å»¶è¿Ÿ
                delay = random.uniform(*self.delays['between_searches'])
                self.logger.info(f"{platform.upper()}æœç´¢å®Œæˆï¼Œè·å–{len(results)}æ¡ç»“æœï¼Œç­‰å¾…{delay:.1f}ç§’")
                await asyncio.sleep(delay)
                
            except Exception as e:
                self.logger.error(f"{platform.upper()} RPAæœç´¢å¤±è´¥: {e}")
                # ç»§ç»­æœç´¢å…¶ä»–å¹³å°è€Œä¸æ˜¯å®Œå…¨å¤±è´¥
                continue
        
        # æŒ‰è´¨é‡å’Œå‚ä¸åº¦æ’åº
        if all_content:
            all_content.sort(key=lambda x: x.confidence_score, reverse=True)
        
        self.logger.info(f"RPAæœç´¢å®Œæˆï¼Œå…±è·å– {len(all_content)} æ¡å†…å®¹")
        return all_content
    
    async def search_google_rpa(self, query: str, time_range: str) -> List[RPAContentItem]:
        """Google RPAé«˜çº§æœç´¢"""
        results = []
        page = await self.context.new_page()
        
        try:
            # è®¿é—®Google
            await page.goto('https://www.google.com', wait_until='networkidle')
            await self.anti_detection.simulate_human_behavior_playwright(page)
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = f'{query} ("side hustle" OR "passive income" OR monetization OR tutorial)'
            
            # è¾“å…¥æœç´¢è¯ - Googleä½¿ç”¨textarea
            search_selectors = ['textarea[name="q"]', 'input[name="q"]', '#APjFqb']
            search_box = None
            
            for selector in search_selectors:
                try:
                    search_box = await page.wait_for_selector(selector, timeout=5000)
                    if search_box:
                        break
                except:
                    continue
            
            if not search_box:
                raise Exception("æœªæ‰¾åˆ°Googleæœç´¢æ¡†")
            
            await search_box.click()
            await page.wait_for_timeout(random.randint(500, 1500))
            
            # æ¨¡æ‹Ÿäººç±»è¾“å…¥
            await search_box.fill('')  # æ¸…ç©º
            await self.anti_detection.type_like_human_playwright(page, search_selectors[0], search_query)
            await page.keyboard.press('Enter')
            
            # ç­‰å¾…ç»“æœåŠ è½½ - ä½¿ç”¨æ›´å®½æ¾çš„é€‰æ‹©å™¨
            await page.wait_for_timeout(3000)
            await self.anti_detection.simulate_human_behavior_playwright(page)
            
            # æå–æœç´¢ç»“æœ - å°è¯•å¤šç§é€‰æ‹©å™¨
            search_results = []
            result_selectors = ['div.g', '.yuRUbf', '[data-ved]', 'div[data-hveid]']
            
            for selector in result_selectors:
                search_results = await page.query_selector_all(selector)
                if search_results:
                    self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                    break
            
            if not search_results:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œå¯èƒ½é‡åˆ°éªŒè¯ç æˆ–è¢«é˜»æ­¢
                page_content = await page.content()
                if 'captcha' in page_content.lower() or 'blocked' in page_content.lower():
                    self.logger.warning("Googleæœç´¢è¢«é˜»æ­¢æˆ–éœ€è¦éªŒè¯ç ")
                else:
                    self.logger.warning("æœªæ‰¾åˆ°æœç´¢ç»“æœå…ƒç´ ")
                return []
            
            for i, result in enumerate(search_results[:self.search_limits['google']]):
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_element = await result.query_selector('h3')
                    link_element = await result.query_selector('a')
                    snippet_element = await result.query_selector('.VwiC3b')
                    
                    if title_element and link_element:
                        title = await title_element.inner_text()
                        url = await link_element.get_attribute('href')
                        snippet = await snippet_element.inner_text() if snippet_element else ''
                        
                        # åˆ›å»ºå†…å®¹é¡¹
                        item = RPAContentItem(
                            platform='google',
                            title=title,
                            content=snippet,
                            url=url,
                            author=self._extract_domain_from_url(url),
                            engagement_metrics={'ranking': len(search_results) - i},
                            confidence_score=self._calculate_google_confidence(title, snippet),
                            scraped_at=datetime.now().isoformat()
                        )
                        
                        results.append(item)
                        
                except Exception as e:
                    self.logger.warning(f"è§£æGoogleç»“æœé¡¹å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            self.logger.error(f"Google RPAæœç´¢å¤±è´¥: {e}")
        finally:
            await page.close()
            
        return results
    
    async def search_youtube_rpa(self, query: str, time_range: str) -> List[RPAContentItem]:
        """YouTube RPAé«˜çº§æœç´¢"""
        results = []
        page = await self.context.new_page()
        
        try:
            # è®¿é—®YouTube
            await page.goto('https://www.youtube.com', wait_until='networkidle')
            await self.anti_detection.simulate_human_behavior_playwright(page)
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = f'{query} side hustle passive income tutorial'
            
            # ç‚¹å‡»æœç´¢æ¡†å¹¶è¾“å…¥
            search_box = await page.wait_for_selector('input#search', timeout=10000)
            await search_box.click()
            await page.wait_for_timeout(random.randint(500, 1500))
            
            await self.anti_detection.type_like_human_playwright(page, 'input#search', search_query)
            
            # ç‚¹å‡»æœç´¢æŒ‰é’®
            search_button = await page.wait_for_selector('button#search-icon-legacy', timeout=5000)
            await search_button.click()
            
            # ç­‰å¾…æœç´¢ç»“æœ
            await page.wait_for_selector('ytd-video-renderer', timeout=15000)
            await self.anti_detection.simulate_human_behavior_playwright(page)
            
            # æå–è§†é¢‘ç»“æœ
            video_elements = await page.query_selector_all('ytd-video-renderer')
            
            for video in video_elements[:self.search_limits['youtube']]:
                try:
                    # æå–è§†é¢‘ä¿¡æ¯
                    title_element = await video.query_selector('a#video-title')
                    channel_element = await video.query_selector('#channel-info #text a')
                    views_element = await video.query_selector('#metadata-line span:first-child')
                    
                    if title_element:
                        title = await title_element.inner_text()
                        video_url = await title_element.get_attribute('href')
                        if video_url:
                            video_url = f'https://www.youtube.com{video_url}'
                        
                        channel = await channel_element.inner_text() if channel_element else 'Unknown'
                        views_text = await views_element.inner_text() if views_element else '0 views'
                        
                        # è§£æè§‚çœ‹æ¬¡æ•°
                        views = self._parse_youtube_views(views_text)
                        
                        # åˆ›å»ºå†…å®¹é¡¹
                        item = RPAContentItem(
                            platform='youtube',
                            title=title,
                            content=f'YouTube video: {title}',
                            url=video_url,
                            author=channel,
                            engagement_metrics={
                                'views': views,
                                'platform_score': min(views / 1000, 100)
                            },
                            confidence_score=self._calculate_youtube_confidence(title, views),
                            scraped_at=datetime.now().isoformat()
                        )
                        
                        results.append(item)
                        
                except Exception as e:
                    self.logger.warning(f"è§£æYouTubeè§†é¢‘å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            self.logger.error(f"YouTube RPAæœç´¢å¤±è´¥: {e}")
        finally:
            await page.close()
            
        return results
    
    async def search_reddit_rpa(self, query: str, time_range: str) -> List[RPAContentItem]:
        """Reddit RPAé«˜çº§æœç´¢"""
        results = []
        subreddits = ['entrepreneur', 'sidehustle', 'passive_income']
        
        for subreddit in subreddits:
            page = await self.context.new_page()
            
            try:
                self.logger.info(f"æ­£åœ¨æœç´¢ r/{subreddit}")
                
                # ä½¿ç”¨è€ç‰ˆæœ¬Redditæ›´å®¹æ˜“è§£æ
                search_url = f'https://old.reddit.com/r/{subreddit}/search/?q={quote(query)}&restrict_sr=1&sort=relevance'
                await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                await page.wait_for_timeout(3000)
                
                # ä½¿ç”¨old.reddit.comçš„ç®€å•é€‰æ‹©å™¨
                post_elements = await page.query_selector_all('.thing')
                
                if not post_elements:
                    self.logger.warning(f"åœ¨ r/{subreddit} æœªæ‰¾åˆ°å¸–å­å…ƒç´ ")
                    continue
                
                self.logger.info(f"åœ¨ r/{subreddit} æ‰¾åˆ° {len(post_elements)} ä¸ªå¸–å­")
                
                for i, post in enumerate(post_elements[:6]):  # æ¯ä¸ªsubredditæœ€å¤š6ä¸ªç»“æœ
                    try:
                        # æå–æ ‡é¢˜ (old reddit)
                        title_element = await post.query_selector('.title a')
                        if not title_element:
                            continue
                        
                        title = await title_element.inner_text()
                        if not title or len(title.strip()) < 5:
                            continue
                        
                        # æå–é“¾æ¥
                        post_url = await title_element.get_attribute('href')
                        if post_url and not post_url.startswith('http'):
                            post_url = f'https://old.reddit.com{post_url}'
                        
                        # æå–ä½œè€…
                        author_element = await post.query_selector('.author')
                        author = await author_element.inner_text() if author_element else 'Unknown'
                        
                        # åˆ›å»ºå†…å®¹é¡¹
                        item = RPAContentItem(
                            platform='reddit',
                            title=title.strip(),
                            content=f'Reddit discussion from r/{subreddit}',
                            url=post_url if post_url else f'https://reddit.com/r/{subreddit}',
                            author=author,
                            engagement_metrics={
                                'subreddit': subreddit,
                                'position': i + 1,
                                'platform_score': 60
                            },
                            confidence_score=self._calculate_reddit_confidence(title),
                            scraped_at=datetime.now().isoformat()
                        )
                        
                        results.append(item)
                        self.logger.debug(f"æå–Redditå¸–å­ {i+1}: {title[:50]}...")
                        
                    except Exception as e:
                        self.logger.warning(f"è§£æRedditå¸–å­ {i+1} å¤±è´¥: {e}")
                        continue
                
                self.logger.info(f"r/{subreddit} æœç´¢å®Œæˆï¼Œè·å– {len([r for r in results if r.engagement_metrics.get('subreddit') == subreddit])} æ¡ç»“æœ")
                
            except Exception as e:
                self.logger.error(f"Reddit {subreddit} æœç´¢å¤±è´¥: {e}")
            finally:
                await page.close()
            
            # subreddité—´çš„å»¶è¿Ÿ
            await asyncio.sleep(random.uniform(2, 4))
        
        return results
    
    # è¾…åŠ©è§£ææ–¹æ³•
    def _extract_domain_from_url(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return 'Unknown'
    
    def _parse_youtube_views(self, views_text: str) -> int:
        """è§£æYouTubeè§‚çœ‹æ¬¡æ•°"""
        try:
            # ç§»é™¤æ–‡å­—ï¼Œåªä¿ç•™æ•°å­—
            views_clean = re.sub(r'[^0-9.]', '', views_text)
            if 'K' in views_text.upper():
                return int(float(views_clean) * 1000)
            elif 'M' in views_text.upper():
                return int(float(views_clean) * 1000000)
            elif 'B' in views_text.upper():
                return int(float(views_clean) * 1000000000)
            else:
                return int(views_clean) if views_clean else 0
        except:
            return 0
    
    # ç½®ä¿¡åº¦è®¡ç®—æ–¹æ³•
    def _calculate_google_confidence(self, title: str, snippet: str) -> float:
        """è®¡ç®—Googleç»“æœç½®ä¿¡åº¦"""
        base_score = 0.7
        
        # æ ‡é¢˜ç›¸å…³æ€§
        if any(keyword in title.lower() for keyword in ['tutorial', 'guide', 'how to', 'step']):
            base_score += 0.15
        
        # å†…å®¹è´¨é‡
        if len(snippet) > 100:
            base_score += 0.1
        
        return min(base_score, 1.0)
    
    def _calculate_youtube_confidence(self, title: str, views: int) -> float:
        """è®¡ç®—YouTubeç»“æœç½®ä¿¡åº¦"""
        base_score = 0.6
        
        # æ ‡é¢˜ç›¸å…³æ€§
        if any(keyword in title.lower() for keyword in ['tutorial', 'guide', 'how to', 'review']):
            base_score += 0.2
        
        # è§‚çœ‹æ¬¡æ•°
        if views > 10000:
            base_score += 0.15
        elif views > 1000:
            base_score += 0.1
        
        return min(base_score, 1.0)
    
    def _calculate_reddit_confidence(self, title: str) -> float:
        """è®¡ç®—Redditç»“æœç½®ä¿¡åº¦"""
        base_score = 0.65
        
        # æ ‡é¢˜ç›¸å…³æ€§
        if any(keyword in title.lower() for keyword in ['guide', 'tips', 'experience', 'success']):
            base_score += 0.15
        
        return min(base_score, 1.0)


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """æµ‹è¯•RPAå¤šå¹³å°çˆ¬è™«"""
    async with RPAMultiPlatformCrawler(use_playwright=True) as crawler:
        results = await crawler.search_all_platforms_rpa("ai automation", "7d")
        
        print(f"\nğŸ¯ RPAæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡å†…å®¹:")
        
        platforms = {}
        for item in results:
            if item.platform not in platforms:
                platforms[item.platform] = []
            platforms[item.platform].append(item)
        
        for platform, items in platforms.items():
            print(f"\nğŸ“Š [{platform.upper()}] - {len(items)}æ¡ç»“æœ:")
            for item in items[:3]:  # æ˜¾ç¤ºæ¯ä¸ªå¹³å°å‰3ä¸ªç»“æœ
                print(f"  âœ“ {item.title[:80]}...")
                print(f"    ä½œè€…: {item.author}")
                print(f"    ç½®ä¿¡åº¦: {item.confidence_score:.2f}")
                print(f"    URL: {item.url}")
                print()

if __name__ == "__main__":
    asyncio.run(main())